\newif\ifshowsolutions
\showsolutionstrue
\input{preamble}
\newcommand{\boldline}[1]{\underline{\textbf{#1}}}

\chead{%
  {\vbox{%
      \vspace{2mm}
      \large
      Machine Learning \& Data Mining \hfill
      Caltech CS/CNS/EE 155 \hfill \\[1pt]
      Miniproject 1\hfill
      Released January $28^{th}$, 2017 \\
    }
  }
}

\usepackage{amsfonts} %Mathbb Fonts
\usepackage{amsmath} %Text in Math
\usepackage{longtable} %Make multi-page tables
\usepackage{enumitem}
\usepackage{graphicx} % Import pics \includegraphics[scale=0.7]{SGD}
\graphicspath{{figures/}} % Change pic path
\usepackage{makecell}

\begin{document}
\pagestyle{fancy}

% LaTeX is simple if you have a good template to work with! To use this document, simply fill in your text where we have indicated. To write mathematical notation in a fancy style, just write the notation inside enclosing $dollar signs$.

% For example:
% $y = x^2 + 2x + 1$

% For help with LaTeX, please feel free to see a TA!



\section{Introduction}
\medskip
\begin{itemize}

    \item \boldline{Group members} \\
    Vaibhav Anand \\
    Nikhil Gupta \\
    Michael Hashe \\
    
    \item \boldline{Team name} \\
    The Breakfast Club
    
    \item \boldline{Division of labour} \\
    % Insert text here.

\end{itemize}



\section{Overview}
\medskip
\begin{itemize}

    \item \boldline{Models and techniques tried}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'. Models and techniques should be bolded using '\textbf{}'.
    \item \textbf{Bullet:} Bullet text.
    \end{itemize}

    \item \boldline{Work timeline}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'.
    \item \textbf{Bullet:} Bullet text.
    \end{itemize}

\end{itemize}



\section{Approach}
\medskip
\begin{itemize}

    \item \boldline{Data processing and manipulation}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'.
    \item \textbf{Bullet:} Bullet text.
    \end{itemize}

    \item \boldline{Details of models and techniques}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'.
    \item \textbf{Bullet:} Bullet text.

    % If you would like to insert a figure, you can just use the following five lines, replacing the image path with your own and the caption with a 1-2 sentence description of what the image is and how it is relevant or useful.
    % \begin{figure}[H]
    % \centering
    % \includegraphics[width=\textwidth]{smiley.png}
    % \caption{Insert caption here.}
    % \end{figure}

    \end{itemize}

\end{itemize}



\section{Model Selection}
\medskip
\begin{itemize}

    \item \boldline{Scoring} \\
    Due to the binary output classification, we chose to do scoring by classification accuracy (as opposed to loss measure). 

    \item \boldline{Validation and Test} \\
    We used cross-5 validation for all model selection and choosing semi-finalists of the neural-network models, except for choosing the finalists in the neural network models, where we used cross-10 validation.  By generating these cross validations, we were able to select the optimal learning parameters, such as regularization terms, early stopping (max iterations), and loss functions. This was done by manually fixing all parameters except for one, optimizing that parameter, then moving on to optimizing another parameter, before circling back to the same parameter. In a way, we performed a manual step-wise gradient-like descent on parameters. The results of cross validation for various models can be seen in Table \# in the Appendix. The parameters variables refer to the API SKlearn's definition of parameters.
    % Insert text here.

\end{itemize}



\section{Conclusion}
\medskip
\begin{itemize}

    \item \boldline{Discoveries} \\
    % Insert text here.

    \item \boldline{Challenges} \\
    % Insert text here.

    \item \boldline{Concluding Remarks} \\
    % Insert text here.

\end{itemize}

\section{Appendix}

\def\arraystretch{1.2}%Line spacing in table, 1 is default
\begin{longtable}{c|c|c|c}
\caption{With data normalization, without de-categorization of data (does not include all tests)}\\\hline
Type & K-Folds & Parameters & Classification Accuracy \\\hline
SVM & 5 & 4-degree polynomial kernel & 0.7568 \\\hline
SVM & 5 & RBF kernel & 0.7568 \\\hline
Logistic Regression & 5 & \makecell*{SAG solver, 25 iterations (converged),\\ $10^{-5}$ regularization strength} & 0.774 \\\hline
Logistic Regression & 5 & \makecell*{SAG solver, 25 iterations (no \\convergence), $C=10^{0}$ regularization strength} & 0.773 \\\hline
Logistic Regression & 5 & \makecell*{SAG solver, 100 iterations (no \\convergence), $C=10^{0}$ regularization strength} & 0.773 \\\hline
Logistic Regression & 5 & \makecell*{SAG solver, 100 iterations (no \\convergence), $C=10^{5}$ regularization strength} & 0.773 \\\hline
Logistic Regression & 5 & \makecell*{SAG solver, 400 iterations (no \\convergence), $C=10^{0}$ regularization strength} & 0.773 \\\hline
Logistic Regression & 5 & \makecell*{Liblinear solver, 100 iterations (no \\convergence), $C=10^{0}$ regularization strength} & 0.774 \\\hline
Ridge Regression & 5 & \makecell*{$\alpha=20$ regularization strength,\\ (optimal alpha found by plotting CV vs. alpha)} & 0.7738 \\\hline
Lasso Regression & 5 & \makecell*{$\alpha=10^{-3}$ regularization strength,\\ (optimal alpha found by plotting CV vs. alpha)} & 0.7718 \\\hline
% Can explain why switched from early stopping tolerance to fixed max iterations
MLP$^{*}$ Classifier & 5 & \makecell*{Hidden layers=(200,100), iterations=5 \\ (optimal iterations found by plotting CV vs. iter.)} & 0.7711 \\\hline
MLP$^{*}$ Classifier & 5 & \makecell*{Hidden layers=(100), iterations=3 \\ (optimal iterations found by plotting CV vs. iter.)} & 0.7726 \\\hline
MLP$^{*}$ Classifier & 5 & \makecell*{Hidden layers=(100, 50, 10), iterations=4 \\ (optimal iterations found by plotting CV vs. iter.)} & 0.7725 \\\hline
MLP$^{*}$ Classifier & 5 & \makecell*{Hidden layers=(20), iterations=10 \\ (optimal iterations found by plotting CV vs. iter.)} & 0.7734 \\\hline
MLP$^{*}$ Classifier & 5 & \makecell*{Hidden layers=(20, 20), iterations=15 \\ (optimal iterations found by plotting CV vs. iter.)} & 0.7733 \\\hline
MLP$^{*}$ Classifier & 5 & \makecell*{Hidden layers=(20, 20,20), iterations=10 \\ (optimal iterations found by plotting CV vs. iter.)} & 0.7730 \\\hline
MLP$^{*}$ Classifier & 5 & \makecell*{Hidden layers=(200,100), iterations=5 \\ (optimal iterations found by plotting CV vs. iter.)} & 0.7711 \\\hline
\caption{Without data normalization, with de-categorization of data}\\\hline
SVM & 5 & RBF kernel & 0.7707 \\\hline
Logistic Regression & 5 & \makecell*{Liblinear solver, 50 iterations (converged) \\ $C=1$ regularization strength} & 0.7725 \\\hline
Logistic Regression & 5 & \makecell*{Liblinear solver, 100 iterations (converged)\\ $C=1$ regularization strength} & 0.7725 \\\hline
SGD & 5 & \makecell*{Hinge loss, 1000 iterations,\\ $\alpha=0.001$ regularization strength} & 0.7748 \\\hline
SGD & 5 & \makecell*{Hinge loss, 500 iterations,\\ $\alpha=0.001$ regularization strength} & 0.7737 \\\hline
SGD & 5 & \makecell*{Hinge loss, 100 iterations,\\ $\alpha=0.001$ regularization strength} & 0.7727 \\\hline
\caption{Finalists from 43 cross validations of various layer dimensions and iterations (bounded cross validation by iterations on both sides, such that an increase or decrease in iterations increased validation significantly):}\\\hline
MLP$^{*}$ Classifier & 10 & \makecell*{Hidden layers=(50, 50), iterations=4 \\ (optimal iterations found by plotting CV vs. iter.)} & 0.7775 \\\hline
MLP$^{*}$ Classifier & 10 & \makecell*{Hidden layers=(100, 50, 10), iterations=9 \\ (optimal iterations found by plotting CV vs. iter.)} & 0.7782 \\\hline
MLP$^{*}$ Classifier & 10 & \makecell*{Hidden layers=(150, 50), iterations=5 \\ (optimal iterations found by plotting CV vs. iter.)} & 0.7776 \\\hline
MLP$^{*}$ Classifier & 10 & \makecell*{Hidden layers=(150, 50, 10), iterations=6 \\ (optimal iterations found by plotting CV vs. iter.)} & 0.7781 \\\hline

\caption{$^{*}$MLP = Multilayered Perceptron. $^{*}$SGD = Stochastic Gradient Descent. We saw de-categorization improve performance significantly in neural network models, whereas linear models performed slightly worse.}
\end{longtable}















\end{document}